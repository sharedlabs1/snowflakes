= Case Study 2: Advanced Analytics and Predictive Modeling Using Snowflake and Python  
Author: Your Name  
:icons: font  
:source-highlighter: pygments  
:toc: preamble  
:numbered:

== Objective
In this case study, you will use Snowflake and Python (through Snowpark) to build an advanced analytics solution. You will perform predictive modeling using machine learning algorithms directly within Snowflake, integrating with external data sources and Python libraries.

== Prerequisites
- Snowflake account access.
- Python environment with Snowflake’s Snowpark installed.
- Sample historical sales data.

== Steps
1. **Creating the Data Warehouse for Analytics**
   . Create a database and schemas for storing historical data and predictions:
   
[source,sql]
   ----
   CREATE OR REPLACE DATABASE analytics_dw;
   CREATE SCHEMA raw_data;
   CREATE SCHEMA predictions;
   ----

2. **Loading Historical Sales Data**
   . Load historical sales data into the **raw_data** schema:
   
[source,sql]
   ----
   CREATE OR REPLACE TABLE raw_data.historical_sales (
     sale_id INT, 
     product_id INT, 
     customer_id INT, 
     sale_date DATE, 
     amount DECIMAL(10, 2), 
     region STRING
   );
   ----
   . Use the COPY INTO command to load data from a stage:
   [source,sql]
   ----
   COPY INTO raw_data.historical_sales FROM @sales_stage FILE_FORMAT = (TYPE = 'CSV');
   ----

3. **Setting Up Snowpark for Python Integration**
   . Install Snowpark for Python in your local Python environment:

[source,cmd]
----
pip install snowflake-snowpark-python
----


4. **Connecting Snowpark to Snowflake**
. Set up a Snowpark session to connect to Snowflake from Python:

[source,python]
----

from snowflake.snowpark import Session
from snowflake.snowpark.functions import col

connection_params = {
  "account": "<your_account>",
  "user": "<your_user>",
  "password": "<your_password>",
  "role": "SYSADMIN",
  "warehouse": "COMPUTE_WH",
  "database": "ANALYTICS_DW",
  "schema": "RAW_DATA"
}

session = Session.builder.configs(connection_params).create()
----
5.Loading Data into a Snowpark DataFrame . Load the historical sales data into a Snowpark DataFrame:

[source,python]
----

df_sales = session.table("historical_sales")
df_sales.show()

----

6.Exploring and Cleaning the Data . Use Snowpark to clean and explore the data:

[source,python]
----
df_clean = df_sales.filter(col("amount") > 0)
df_clean.show()

----

7.Creating Feature Columns for Modeling . Generate additional features (e.g., month, year, product category) for the predictive model:

[source,python]
----
df_features = df_clean.with_column("sale_month", col("sale_date").month())
df_features.show()


----

8.Training a Predictive Model in Snowflake . Use Python’s Scikit-learn library to train a regression model on the sales data:

[source,python]
----
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

sales_data = df_features.to_pandas()
X = sales_data[["sale_month", "product_id"]]
y = sales_data["amount"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = LinearRegression()
model.fit(X_train, y_train)

----
9.Generating Predictions . Use the trained model to generate predictions for the test data:

[source,python]
----
predictions = model.predict(X_test)


----

10.Saving Predictions to Snowflake . Convert the predictions into a DataFrame and store them in Snowflake:

[source,python]
----
import pandas as pd

df_predictions = pd.DataFrame({
  "product_id": X_test["product_id"],
  "sale_month": X_test["sale_month"],
  "predicted_amount": predictions
})

session.write_pandas(df_predictions, "predictions.sales_forecast")

----

11.Creating a Predictions Table in Snowflake . Create a new table to store the predictions: 

[source,sql]
----
CREATE OR REPLACE TABLE predictions.sales_forecast ( product_id INT, sale_month INT, predicted_amount DECIMAL(10,2) );
----

12.Analyzing Prediction Accuracy . Use Snowpark to calculate the accuracy of the predictions:

[source,python]
----
from sklearn.metrics import mean_squared_error

y_pred = model.predict(X_test)
error = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", error)

----

13.Visualizing Predictions . Use Matplotlib to visualize the predicted vs actual sales:

[source,python]
----
import matplotlib.pyplot as plt

plt.scatter(y_test, predictions)
plt.xlabel("Actual Sales")
plt.ylabel("Predicted Sales")
plt.show()


----

14.Building a Time-Series Forecasting Model . Train a time-series forecasting model on the historical sales data using ARIMA:

[source,python]
----
from statsmodels.tsa.arima.model import ARIMA

arima_model = ARIMA(y_train, order=(5, 1, 0))
arima_model_fit = arima_model.fit()
forecast = arima_model_fit.forecast(steps=12)

----

15. Saving Time-Series Forecasts . Save the time-series forecasts to Snowflake for future reporting: 

[source,sql]
----
INSERT INTO predictions.sales_forecast VALUES (102, 11, forecast[0]), (103, 12, forecast[1]);
----

16.Integrating External Data Sources . Combine external economic data (e.g., exchange rates) with sales data to improve the predictive model:

[source,sql]
----
CREATE OR REPLACE TABLE external_data ( date DATE, exchange_rate DECIMAL(10,2) );
INSERT INTO external_data VALUES ('2023-10-01', 1.15);
----

17.Joining External Data with Predictions . Join the external data with the sales_forecast table for more accurate predictions:

[source,sql]
----
SELECT p.*, e.exchange_rate FROM predictions.sales_forecast p JOIN external_data e
ON p.sale_month = EXTRACT(MONTH FROM e.date);

----

18. Storing Historical Models for Future Use . Store trained models as UDFs in Snowflake for future reuse:

[source,sql]
----
import joblib
joblib.dump(model, "sales_forecast_model.pkl")

----

19.Creating a UDF to Predict Sales . Create a Snowflake UDF that uses the stored model to predict future sales:

[source,sql]
----
CREATE OR REPLACE FUNCTION predict_sales(product_id INT, sale_month INT)
RETURNS DECIMAL(10,2) LANGUAGE PYTHON RUNTIME_VERSION = '3.8' HANDLER = 'sales_forecast_model.pkl';
----

20.Using the UDF for Predictions . Use the UDF to predict sales directly in Snowflake: 

[source,sql]
----
SELECT product_id, sale_month, predict_sales(product_id, sale_month) FROM predictions.sales_forecast;
----

21.Automating Predictive Updates . Create a task to automatically update the predictions every month: 

[source,sql]
----
CREATE OR REPLACE TASK update_sales_forecast WAREHOUSE = 'COMPUTE_WH' SCHEDULE = '1 MONTH' 
AS CALL predict_sales(product_id, sale_month);
----

22.Visualizing the Forecasts . Integrate Snowflake data with a BI tool like Tableau to visualize the predicted sales trends over time.

23.Monitoring Model Performance . Monitor the model’s performance by comparing predicted values to actual sales data in Snowflake: 

[source,sql]
----
SELECT * FROM predictions.sales_forecast WHERE predicted_amount <> actual_amount;

----

24.Updating the Model with New Data . Retrain the model with new data as it becomes available and store the updated model in Snowflake.

Implementing Forecast Alerts . Set up alerts to notify the sales team when predicted sales fall below a certain threshold:

[source,sql]
----
SELECT product_id, predicted_amount FROM predictions.sales_forecast WHERE predicted_amount < 100;
----

25.Securing Access to Predictions . Secure the predictions schema by granting access only to authorized roles: 

[source,sql]
----
GRANT USAGE ON SCHEMA predictions TO ROLE data_scientist;
GRANT SELECT ON ALL TABLES IN SCHEMA predictions TO ROLE data_scientist;

----

26.Testing Row-Level Security for Model Outputs . Implement row-level security to ensure that predictions for different regions are only visible to regional managers: 

[source,sql]
----
CREATE SECURE VIEW predictions.secure_sales_forecast AS 
    SELECT * FROM predictions.sales_forecast WHERE region = CURRENT_ROLE();

----

27.Using Snowflake Streams to Handle New Data . 
Set up a stream to track new sales records added to the historical_sales table:

[source,sql]
----
CREATE OR REPLACE STREAM sales_stream ON TABLE raw_data.historical_sales;

----

28.Processing New Data for Predictions . Create a task to automatically process new data and update predictions:

[source,sql]
----
CREATE OR REPLACE TASK process_new_sales_data WAREHOUSE = 'COMPUTE_WH' SCHEDULE = '1 HOUR' 
 AS CALL predict_sales(new_data.product_id, new_data.sale_month);
----

29.Creating a Final Report for Stakeholders .

Use Snowflake’s integration with Tableau or Power BI to create a final report that displays predictive sales trends and model accuracy for stakeholders.

== Conclusion

You have built a complete advanced analytics and predictive modeling solution using Snowflake and Python, handling data ingestion, feature engineering, model training, predictions, and reporting.
