= Lab 11: Bulk Data Loading into Snowflake  
Author: Your Name  
:icons: font  
:source-highlighter: pygments  
:toc: preamble  
:numbered:

== Objective
In this lab, you will perform bulk data loading into Snowflake using the **COPY INTO** command, including loading large datasets from external cloud storage.

== Prerequisites
- Snowflake account access.
- Access to an AWS S3 bucket or Azure Blob Storage containing a large dataset.

== Steps
1. **Creating a Stage for External Storage**
   . Create an external stage to connect to your cloud storage:

[source,sql]
----
CREATE OR REPLACE STAGE my_stage URL='s3://your-bucket-name/path/' 
CREDENTIALS=(AWS_KEY_ID='your_key_id' AWS_SECRET_KEY='your_secret_key');
----


2. **Loading Data from S3 to Snowflake**
. Create a table in Snowflake to load data into:

[source,sql]
----
CREATE OR REPLACE TABLE bulk_data_table (id INT, name STRING, age INT);
----

. Use the **COPY INTO** command to load the data from the external stage:

[source,sql]
----
COPY INTO bulk_data_table FROM @my_stage FILE_FORMAT = (TYPE = 'CSV');
----


3. **Verifying Bulk Load**
. Verify the data load by querying the **bulk_data_table**:

[source,sql]
----
SELECT COUNT(*) FROM bulk_data_table;
----


4. **Monitoring the Load**
. Use the **LOAD_HISTORY** view to monitor the status of the bulk data load:

[source,sql]
----
SELECT * FROM LOAD_HISTORY WHERE TABLE_NAME = 'bulk_data_table';
----


== Conclusion
- You have successfully loaded bulk data into Snowflake using the COPY INTO command from an external cloud storage service.


