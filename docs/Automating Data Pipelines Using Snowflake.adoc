= Lab 30: Automating Data Pipelines Using Snowflake Tasks, Streams, and Procedures  
Author: Your Name  
:icons: font  
:source-highlighter: pygments  
:toc: preamble  
:numbered:

== Objective
In this lab, you will automate complex data pipelines in Snowflake using tasks, streams, and stored procedures to create an end-to-end data workflow.

== Prerequisites
- Snowflake account access.

== Steps
1. **Creating the Source Table**
   . Create a source table to simulate incoming data:
  
[source,sql]
   ----
   CREATE TABLE incoming_orders (
     order_id INT,
     product_id INT,
     order_date DATE,
     status STRING
   );
   ----

2. **Inserting Test Data**
   . Insert test data into the source table:
  
[source,sql]
   ----
   INSERT INTO incoming_orders VALUES 
   (1, 101, '2023-10-01', 'PENDING'),
   (2, 102, '2023-10-02', 'SHIPPED');
   ----

3. **Creating a Stream to Track Changes**
   . Create a stream to track changes in the `incoming_orders` table:
   
[source,sql]
   ----
   CREATE STREAM incoming_orders_stream ON TABLE incoming_orders;
   ----

4. **Creating a Procedure to Process Data**
   . Create a stored procedure to process new data from the stream:
   
[source,sql]
   ----
   CREATE OR REPLACE PROCEDURE process_new_orders()
   RETURNS STRING
   LANGUAGE SQL
   AS
   $$
     INSERT INTO processed_orders (order_id, product_id, order_date, status)
     SELECT order_id, product_id, order_date, status
     FROM incoming_orders_stream
     WHERE METADATA$ACTION = 'INSERT';
   $$;
   ----

5. **Creating the Processed Orders Table**
   . Create the table to store processed orders:
  
[source,sql]
   ----
   CREATE TABLE processed_orders (
     order_id INT,
     product_id INT,
     order_date DATE,
     status STRING
   );
   ----

6. **Creating a Task to Automate the Procedure**
   . Create a task to run the `process_new_orders` procedure every hour:
  
[source,sql]
   ----
   CREATE OR REPLACE TASK order_processing_task
   WAREHOUSE = 'COMPUTE_WH'
   SCHEDULE = '1 HOUR'
   AS
   CALL process_new_orders();
   ----

7. **Executing the Task Manually**
   . Execute the task manually to process the current data:
   
[source,sql]
   ----
   EXECUTE TASK order_processing_task;
   ----

8. **Verifying Processed Data**
   . Query the `processed_orders` table to verify that data was processed:
   
[source,sql]
   ----
   SELECT * FROM processed_orders;
   ----

9. **Inserting Additional Data**
   . Insert more data into the `incoming_orders` table to simulate new incoming orders:
   
[source,sql]
   ----
   INSERT INTO incoming_orders VALUES (3, 103, '2023-10-03', 'PENDING');
   ----

10. **Checking the Stream for New Changes**
   . Query the stream to see the new records:
  
[source,sql]
   ----
   SELECT * FROM incoming_orders_stream;
   ----

11. **Running the Task Automatically**
   . Wait for the task to execute automatically according to its schedule or trigger it manually:
   
[source,sql]
   ----
   EXECUTE TASK order_processing_task;
   ----

12. **Logging Task Execution History**
   . Query the **TASK_HISTORY** view to see the execution history of the task:
  
[source,sql]
   ----
   SELECT * FROM TASK_HISTORY WHERE TASK_NAME = 'ORDER_PROCESSING_TASK';
   ----

13. **Handling Task Failures**
   . Simulate a task failure by altering the procedure to introduce an error, then query the **TASK_HISTORY** for errors:
  
[source,sql]
   ----
   CREATE OR REPLACE PROCEDURE process_new_orders()
   RETURNS STRING
   LANGUAGE SQL
   AS
   $$
     INSERT INTO non_existent_table (order_id, product_id, order_date, status)
     SELECT order_id, product_id, order_date, status
     FROM incoming_orders_stream;
   $$;
   ----

14. **Fixing the Task and Reprocessing**
   . Correct the procedure and reprocess the data:
  
[source,sql]
   ----
   CREATE OR REPLACE PROCEDURE process_new_orders()
   RETURNS STRING
   LANGUAGE SQL
   AS
   $$
     INSERT INTO processed_orders (order_id, product_id, order_date, status)
     SELECT order_id, product_id, order_date, status
     FROM incoming_orders_stream;
   $$;
   EXECUTE TASK order_processing_task;
   ----

15. **Optimizing the Workflow**
   . Add logic to the procedure to handle updates or deletions:
   [source,sql]
   ----
   CREATE OR REPLACE PROCEDURE process_new_orders()
   RETURNS STRING
   LANGUAGE SQL
   AS
   $$
     MERGE INTO processed_orders AS t
     USING incoming_orders_stream AS s
     ON t.order_id = s.order_id
     WHEN MATCHED THEN UPDATE SET t.status = s.status
     WHEN NOT MATCHED THEN INSERT (order_id, product_id, order_date, status)
     VALUES (s.order_id, s.product_id, s.order_date, s.status);
   $$;
   ----

== Conclusion
- You have successfully automated an end-to-end data pipeline in Snowflake using tasks, streams, and stored procedures, handling incoming data and processing it automatically.
